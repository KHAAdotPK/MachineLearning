# Machine Learning

## Neural Network Fundamentals: From Theory to Implementation

##### Project Overview.
This repository contains a series of in-depth technical articles exploring the mathematical foundations and practical implementations of neural network architectures. The content bridges the gap between theoretical understanding and code implementation, providing readers with both rigorous mathematical derivations and corresponding implementation details.

### Articles in this Series
[Fully Connected Passes.](./fully_connected_passes.txt)
A comprehensive exploration of forward and backward passes in fully connected neural networks, including:

- Detailed matrix operations with shape analysis
- Step-by-step breakdown of computational flow
- Precise mathematical formulations with practical C-style notation
- Complete derivation of gradient calculations for backpropagation

[Fully Connected Passes in Transformer Attention Layers.](./fully_connected_passes_in_transformer_attention_layer.txt)
An advanced examination of how traditional neural network operations extend to modern transformer architectures:

- Connection between standard neural network operations and self-attention mechanisms
- Detailed breakdown of Query, Key, and Value projections
- Implementation of scaled dot-product attention with proper scaling factors
- Comprehensive backward pass derivations showing gradient flow through complex attention operations
- C++ implementation with extensive documentation and implementation notes


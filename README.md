# Machine Learning.

## Neural Network Fundamentals: From Theory to Implementation

##### Project Overview.
This repository contains a series of in-depth technical articles exploring the mathematical foundations and practical implementations of neural network architectures. The content bridges the gap between theoretical understanding and code implementation, providing readers with both rigorous mathematical derivations and corresponding implementation details.

### Articles in this Series

[A Simple Neural Network](./a_simple_neural_network.md)
This introductory article demystifies **neural networks** by breaking down their fundamental structure and operations

- It explains how a basic **neural network** with one hidden layer processes information, using a concrete example with specific dimensions
- Through clear explanations of the **forward pass**, **matrix operations**, and data transformations, readers will gain a solid foundation in **neural network** concepts without requiring advanced mathematical knowledge
- Ideal for beginners and those seeking to refresh their understanding of **neural network** basics

[Fully Connected Passes.](./fully_connected_passes.txt)
A comprehensive exploration of forward and backward passes in fully connected neural networks, including:

- Detailed matrix operations with shape analysis
- Step-by-step breakdown of computational flow
- Precise mathematical formulations with practical C-style notation
- Complete derivation of gradient calculations for backpropagation

[Fully Connected Passes in Transformer Attention Layers.](./fully_connected_passes_in_transformer_attention_layer.txt)
An advanced examination of how traditional neural network operations extend to modern transformer architectures:

- Connection between standard neural network operations and self-attention mechanisms
- Detailed breakdown of Query, Key, and Value projections
- Implementation of scaled dot-product attention with proper scaling factors
- Comprehensive backward pass derivations showing gradient flow through complex attention operations
- C++ implementation with extensive documentation and implementation notes

[Transformer Input Sequence Analysis.](https://github.com/KHAAdotPK/Transformer-Encoder-Decoder/blob/main/Implementation/ML/NLP/transformers/encoder-decoder/DOCUMENTS/input_sequence_analysis.md)

[Transformer Position Encoding Analysis.](https://github.com/KHAAdotPK/Transformer-Encoder-Decoder/blob/main/Implementation/ML/NLP/transformers/encoder-decoder/DOCUMENTS/position_encoding_analysis.md)
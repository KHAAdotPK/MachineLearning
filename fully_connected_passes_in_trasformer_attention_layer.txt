/*
	fully_connected_passes_in_transformer_attention_layer.txt
	Written by, Sohail Qayum Malik.
	
	This article provides a detailed exploration of the forward and backward passes within a Transformer's self-attention mechanism.
	It elucidates the mathematical foundations of each pass, drawing parallels to fundamental neural network operations.
	Furthermore, it offers a practical C++ implementation of the forward pass and a comprehensive,
	step-by-step breakdown of the backward pass, highlighting crucial aspects like scaling,
	masking, and the caching of intermediate values for efficient gradient computation.
	This resource aims to demystify the inner workings of the attention layer, a core component of the Transformer architecture. 	
 */

Lets relate the detail of forward and backward passes to the Transformer attention layer.

1. Find forward pass equation (what's analogous to Y = X.W^T + b) in attention layer:

// --------------------------------------------------
// SECTION 1: Theoretical Equations for Forward Pass
// --------------------------------------------------

- We comute Quey(Q), Key(K), Value(V):
  - Q = X . W^Q 
  - K = X . W^K
  - V = X . W^V

Then
- Attention scores: S = Q.K^T/sqrt(d_k)
- Attention weights A = softmax(S)
- Output before projection O = A . V
- Final output Y = O . W^O // This final output Y is in the simplified equation: output = input × weights, possibly with a bias(many frameworks include them; some simplified implementations skip them).

This (Y = O . W^O) resembles the common neural network transformation: Y = XW^T + b (bias often omitted in attention layers), highlighting that attention can also be viewed as a series of linear layers followed by weighted combination through softmax.

// --------------------------------------------------------
// SECTION 2: C++ Implementation of Attention Forward Pass
// --------------------------------------------------------

Below is the implementation of the forward pass for a single-head attention mechanism in C++, where each input token is independently transformed into query, key, and value vectors. These are then used to compute scaled dot-product attention, followed by an output projection.

```C++

        Collective<t> forward(Collective<t>& ei_query, Collective<t>& ei_key, Collective<t>& ei_value, Collective<t>& mask)
        {                          
            /*
                Linear transformations, compute queries, keys, and values
             */
             
            /*
                It makes sense to use singular names (query, key, value) because:
                - Each line is processed independently (not as a batch).
                - Each token gets transformed into a query, key, and value vector separately before attention is applied.

                The "scores" matrix will always be square if, ei_query and ei_key has the same shape... 
                Which is true in the case of this implementation and...
                that is why we can do away with just a query mask and we do not need a separate query mask 
             */
            Collective<t> query, key, value, scores;
            /*
                It makes sense to keep scores and attention_weights in plural form because:
                - Each query attends to multiple keys → The result is a matrix of scores.
                - Softmax produces multiple attention weights (one for each query-key pair).
             */
            Collective<t> attention_weights;

            Collective<t> output;

            X_ei_query = ei_query; // Cache the input for later use in backward pass
            X_ei_key = ei_key;     // Cache the input for later use in backward pass
            X_ei_value = ei_value; // Cache the input for later use in backward pass

            // this->cached_value = ei_value; // Cache the value for later use in backward pass
            
            try
            {

                /*
                    Use one and only one of the following scaling strategies:

                    1. Option 1: Scale Q and K during projections:
                        - Q = X * W^Q / sqrt(d_k)
                        - K = X * W^K / sqrt(d_k)
                        - V = X * W^V (no scaling needed in either case)
                        scores = query · key^T;
                    
                    2. Option 2: Scale scores after computing them:
                        - Q = X * W^Q
                        - K = X * W^K
                        - V = X * W^V (no scaling needed in either case)
                        scores = query · key^T / sqrt(d_k);    
                 */

                /*
                    (where X is the input to the MHA(Multi-Head Attention) layer, the one used for the value projection)
                 */

                /**********************************************************************************************************************************************************/
                /*Note: Only query and key are scaled by the square root of the head dimension (d_k) in the forward pass                                                  */
                /*      because the attention scores are computed as the dot product of query and key.                                                                    */
                /*      This scaling prevents the dot-product values from growing too large in magnitude, which would push softmax into regions with very small gradients.*/
                /**********************************************************************************************************************************************************/ 
                // Q: XW^Q, X is the input to the MHA layer(a.k.a ei_query)                
                query = Numcy::matmul<t>(ei_query, queryWeights) * scaleFactor;
                // K: XW^K, X is the input to the MHA layer(a.k.a ei_key) 
                key = Numcy::matmul<t>(ei_key, keyWeights) * scaleFactor;
                // V: XW^V, X is the input to the MHA layer(a.k.a ei_value)                
                value = Numcy::matmul<t>(ei_value, valueWeights); // No scaling for V

                /* I have checked with ADHOC_DEBUG_MACRO for the first run of above three functions their outputs keep the padding rows */
                
                /*
                    Masking has to be consistently applied in both forward and backward passes to avoid leaking gradient through padded tokens.                  
                    Zero out padded rows in the projected value matrix (after matmul(ei_value, valueWeights) but before attention)

                    Note:-
                    Scores corresponding to masked tokens should be set to -inf (or a very negative number) before softmax so they get zero weight.
                 */
                for (cc_tokenizer::string_character_traits<char>::size_type k = 0; k < value.getShape().getDimensionsOfArray().getNumberOfInnerArrays(); k++)
                {
                    if (mask[k] == 0)
                    {
                        for (cc_tokenizer::string_character_traits<char>::size_type l = 0; l < value.getShape().getNumberOfColumns(); l++)
                        {
                            query[k*value.getShape().getNumberOfColumns() + l] = /*std::numeric_limits<t>::lowest()*/ 0;
                            key[k*value.getShape().getNumberOfColumns() + l] = /*std::numeric_limits<t>::lowest()*/ 0;
                            value[k*value.getShape().getNumberOfColumns() + l] = /*std::numeric_limits<t>::lowest()*/ 0;
                        }
                    }
                }

                // Cache the transformed Q, K, V for backward pass
                /*
                    Make sure that it is the same value which is used in final attention projection output
                    O = A · V
                 */
                this->masked_cached_value = value;

                this->masked_cached_query = query;
                this->masked_cached_key = key;

               
                // *************************************** //
                //  Proceed with attention calculation...  //
                // *************************************** //

                /* Compute scaled dot-product attention scores */
                scores = Numcy::matmul<t>(query, Numcy::transpose(key)); 
                static_assert(std::is_same<cc_tokenizer::allocator<double>, cc_tokenizer::allocator<double>>::value, "Double allocator specialization missing");

                /* ********************************************** */
                /* IT IS HERE JUST FOR THE DOCUMENTATION PURPOSES */
                /* ********************************************** */
                /**
                 * WORKAROUND IMPLEMENTATION FOR SCALAR DIVISION
                 * 
                 * Original Issue:
                 * - The template operator/(F x) that uses cc_tokenizer::allocator fails despite:
                 *   1. Confirmed allocator<double> specialization exists (static_assert passes)
                 *   2. scaleFactor is verified to be of type double (typeid shows 'd')
                 * - The root cause appears to be template instantiation/visibility issue in complex inheritance chain
                 *
                 * Current Solution:
                 * 1. Creates a temporary Collective<t> with shape [1,1] initialized to zeros
                 *    - Uses Numcy::zeros instead of allocator to avoid template issues
                 *    - Explicitly sets the single element to scaleFactor value
                 * 2. Uses existing Collective<t>/Collective<t> operator
                 *
                 * Advantages:
                 * - Avoids problematic allocator path entirely
                 * - Uses already tested/working matrix division
                 * - Maintains numerical consistency with other operations
                 *
                 * Trade-offs:
                 * - Slightly less efficient than direct scalar division:
                 *   - Allocates temporary matrix (though small)
                 *   - Uses full matrix division machinery
                 * - Requires scaleFactor to be convertible to type t
                 *
                 * Future Improvements:
                 * 1. Could implement optimized scalar division operator later:
                 *    template<typename t>
                 *    Collective<t> operator/(t scalar) { element-wise division }
                 * 2. Should investigate why allocator path fails despite proper specialization
                 *
                 * Debugging Notes:
                 * - Verified working for float/double cases
                 * - Maintains proper dimensionality in output
                 * - Preserves exception safety guarantees
                 */
                /* // Collective<t> divisor = Numcy::zeros<t>(DIMENSIONS{1, 1, NULL, NULL});    
                   // divisor[0] = scaleFactor;
                   // scores = scores / divisor;*/
                /* // scores = scores / static_cast<double>(scaleFactor);
                   // std::cout << "Type of scaleFactor: " << typeid(decltype(scaleFactor)).name() << std::endl;*/

                ADHOC_IMPLEMENTATION_OF_MASK_QUERY(scores, mask, false);
                ADHOC_IMPLEMENTATION_OF_MASK_KEY(scores, mask, false);

                /* ADHOC_DEBUG_MACRO(scores); */
                
                /*
                    Do You Need src_mask?
                    If input sequences are of equal length and don't have padding, then src_mask might not be meeded. However, it's best to support it for flexibility later.

                    In a Transformer encoder, src_mask (source mask) is typically used in the self-attention mechanism to:
                    1. Prevent attending to padding tokens (mask out padded positions in the input).
                    2. Control which tokens can attend to which (if needed, like in some structured data cases).

                    What You Need to Do?
                    If you're using matmul(Q, K^T), apply the mask before softmax:
                    attention_scores = attention_scores + src_mask;  // Apply mask  

                    Make sure src_mask has negative infinity (-inf) where padding exists, so softmax turns those values into 0.

                    Check Attention Class:
                    If attention implementation already accepts a mask parameter, pass src_mask from the encoder when calling forward()
                 */
                
                 /*
                    - A Attention weights, which are the normalized scores indicating how much focus each word should receive.
                        These weights are sometimes called just "attention weights"  and other times are called "cached attention weights"
                  */    
                // Apply softmax to get (attention weights a.k.a "A")  
                attention_weights = softmax<t>(scores);
                
                /*
                    - A cached
                      Attention weights, which are the normalized scores indicating how much focus each word should receive.
                      These weights are sometimes called just "attention weights"  and other times are called "cached attention weights"
                 */
                this->cached_attention_weights = attention_weights;
                
                /*
                    Multiply by value
                    O = A · V
                 */
                output = Numcy::matmul<t>(attention_weights, value);                                
                /*
                    - O  
                      Output from attention before output projection
                 */
                this->cached_output_before_projection = output;
                
                /*
                     Final Projected Output: Attention Projection Output = O*Wo = OWo Matrix
                     Y = O · Wo

                    - O
                      Output from attention before output projection (a.k.a "output")
                    - Wo 
                      Output projection weights (a.k.a "outputWeights")
                    
                    Let Y = O*Wo = OWo Matrix (a.k.a "Output matrix")
                    In Step-1 of the backward pass, we have dL/dY = incoming_gradient when Y = OWo
                 */
                output = Numcy::matmul<t>(output, outputWeights);                
            }
            catch(ala_exception& e)
            {
                throw ala_exception(cc_tokenizer::String<char>("Attention::forward() -> ") + cc_tokenizer::String<char>(e.what()));
            }
            
            return output;
        }

```

2. Backward pass in simple terms:

We begin with a loss L that depends on some output Y, which itself was computed during the forward pass as: Y = X . W^T + b.

// ---------------------------------------------------
// SECTION 1: Theoretical Equations for Backward Pass
// ---------------------------------------------------

In the backward pass, we want to know ...

- How much the loss changes if we change W? => This is dL/dW = X^T · dL/dY
- How much the loss changes if we change X? => This is dL/dX = dL/dY . W 

These appear multiple times in the attention code:

- When computing gradients for W^Q, W^K, W^V, and W^O
- When computing dL/dX as sum of partial derivatives from Q, K, and V paths

Think of the backward pass like peeling back the layers of the forward pass, asking:

"What had to change earlier in order to cause this change in the final loss?"

You reverse the operations (matmuls, softmax, projections) and compute how much each one influenced the final loss, step by step. Each step just applies the chain rule.

// ---------------------------------------------------------
// SECTION 2: C++ Implementation of Attention Backward Pass
// ---------------------------------------------------------

```C++

        /**
         * @brief Backward pass for the Multi-Head Attention mechanism.
         * 
         * This function implements the complete backpropagation logic for the attention layer.
         * Given the incoming gradient from the next layer (dL/dY), it computes gradients with respect to:
         * - Output projection weights (W^O)
         * - Attention weights (A)
         * - Value vectors (V), Key vectors (K), and Query vectors (Q)
         * - Their respective projection weights (W^V, W^K, W^Q)
         * - The original input X (used for Q, K, V projections)
         * 
         * Steps followed:
         * 1. Compute dL/dW^O using the cached output before projection.
         * 2. Backpropagate to get dL/dO (gradient w.r.t. attention output).
         * 3. From dL/dO, compute:
         *    - dL/dA (attention weights)
         *    - dL/dV (value vectors)
         * 4. Apply softmax derivative to get dL/dS (attention scores).
         * 5. Use dL/dS to compute:
         *    - dL/dK (key vectors)
         *    - dL/dQ (query vectors)
         * 6. Use gradients from Q, K, and V to compute:
         *    - dL/dW^Q
         *    - dL/dW^K
         *    - dL/dW^V
         * 7. Update weights (Q, K, V) using gradients and learning rate.
         * 8. Compute gradients with respect to the original input X, by propagating gradients backward
         *    through the linear projection layers.
         * 
         * @param incoming_gradient The gradient from the next layer (∂L/∂Y).
         * @param learning_rate The learning rate used for weight updates.
         * @return Gradient with respect to the input of the attention layer (∂L/∂X).
         * 
         * @throws ala_exception if any step of the computation fails.
         */
        Collective<t> backward(Collective<t>& incoming_gradient, t learning_rate = ATTENTION_LAYER_DEFAULT_LEARNING_RATE) throw (ala_exception)
        {
            Collective<t> input_gradient; // Gradient with respect to the input tensors (queries, keys, values) and the weights.

            /*
                The backward pass of the attention mechanism involves computing gradients with respect to the input tensors (queries, keys, values) and the weights.
                This is typically done using backpropagation through the attention mechanism.
             */

            try 
            {   
                /*  
                    1. Gradient of Loss w.r.t. Output Projection Weights (Wo), dL/dWo = O^T * dL/dY
                    Where O = cached_output_before_projection, Wo = outputWeights, dL/dY = incoming_gradient when Y = OWo
                 */
                Collective<t> gradient_output_weights = Numcy::matmul<t>(Numcy::transpose(cached_output_before_projection), incoming_gradient);

                /*
                    2. Gradient of Loss w.r.t. Attention Output (O), dL/dO = dL/dY * Wo^T
                    where Wo is the "output projection weights", dL/dY is the "final Projected Output" (a.k.a incoming_gradient) 
                    therefore, dL/dO is the gradient of the loss with respect to the attention output (a.k.a gradient_attention_output)
                 */
                Collective<t> gradient_attention_output = Numcy::matmul<t>(incoming_gradient, Numcy::transpose(this->outputWeights));

                /*
                    3. Gradient of Loss w.r.t. Attention Weights (A), dL/dA = dL/dO * V^T
                    where V is the "value weights", we must use exactly the same V that was used in computing the attention output(forward pass) O = A * V
                    and then dL/dO is the "gradient_attention_output" of step 2 
                    therefore, dL/dA is the "gradient of the loss with respect to the attention weights" (a.k.a gradient_attention_weights)
                 */
                Collective<t> gradient_attention_weights = Numcy::matmul<t>(gradient_attention_output, Numcy::transpose(this->masked_cached_value));

                /*                    
                    4. Gradient Loss w.r.t. Value Vector (V = X.W^V), dL/dV = A^T * dL/dO
                    where A is the attention weights(a.k.a cached_attention_weights or just attention_weights), dL/dO is the gradient_attention_output
                 */
                Collective<t> gradient_value = Numcy::matmul<t>(Numcy::transpose(this->cached_attention_weights), gradient_attention_output);

                /*
                    5. Gradient of Loss w.r.t. Attention Scores, dL/dS = dL/dA * softmax'(A) (a.k.a softmax_backward(A))
                    where A is the attention weights(a.k.a cached_attention_weights), dL/dA is the "gradient_attention_weights" 
                    herefore, dL/dS is the "gradient of the loss with respect to the attention scores" (a.k.a gradient_attention_scores)
                 */
                Collective<t> gradient_attention_scores = softmax_backward(gradient_attention_weights, this->cached_attention_weights);

                /*
                    6. Gradient of Loss w.r.t. Key Vector (K = X.W^K), dL/dK = 1/sqrt(d_k) * ((dL/dS)^T * Q)
                    where Q is the query weights(a.k.a cached_query), dL/dS is the gradient_attention_scores and 1/sqrt(d_k) is the scaling factor(a.k.a scaleFactor)
                    herefore, dL/dK is the gradient of the loss with respect to the key vectors (a.k.a gradient_key)
                    6.1 => dL/dK = (dL/dS)^T * Q 
                    6.2 => dL/dK = dL/dK * scaleFactor
                 */                
                Collective<t> gradient_key = Numcy::matmul<t>(Numcy::transpose(gradient_attention_scores), this->masked_cached_query);
                gradient_key = gradient_key * scaleFactor;
                
                /*
                    7. Gradient of Loss w.r.t. Query Vector (Q = X.W^Q), dL/dQ = 1/sqrt(d_k) * ((dL/dS)^T * K)
                    where K is the key weights(a.k.a cached_key), dL/dS is the gradient_attention_scores and 1/sqrt(d_k) is the scaling factor(a.k.a scaleFactor)
                    herefore, dL/dQ is the gradient of the loss with respect to the query vectors (a.k.a gradient_query)
                    7.1 => dL/dQ = (dL/dS)^T * K 
                    7.2 => dL/dQ = dL/dQ * scaleFactor
                 */
                Collective<t> gradient_query = Numcy::matmul<t>(Numcy::transpose(gradient_attention_scores), this->masked_cached_key);
                gradient_query = gradient_query * scaleFactor; 
                
                /*
                    --------------------------------------------------------------------------------------------------------------------------------- 
                   | Finally.                                                                                                                        |  
                   | These three following steps are the correct final gradient calculations for the weights of the multi-head attention (MHA) layer |     
                    ---------------------------------------------------------------------------------------------------------------------------------
                 */
                /*
                    8. Gradient of Loss w.r.t. Query Weights (W^Q), dL/dW^Q = X^T * dL/dQ
                    where X is the input to the MHA layer, W^Q is projection matrix for Q(a.k.a queryWeights),
                    dL/dQ is the gradient_query(calculated in step 7)
                 */ 
                Collective<t> gradient_query_weights = Numcy::matmul<t>(Numcy::transpose(this->X_ei_query), gradient_query);
                /*
                    9. Gradient of Loss w.r.t. Key Weights (W^K), dL/dW^K = X^T * dL/dK
                    where X is the input to the MHA layer, W^K is projection matrix for K(a.k.a keyWeights),
                    dL/dK is the gradient_key(calculated in step 6)
                 */
                Collective<t> gradient_key_weights = Numcy::matmul<t>(Numcy::transpose(this->X_ei_key), gradient_key);
                /*  
                    10. Gradient of Loss w.r.t. Value Weights (W^V), dL/dW^V = X^T * dL/dV
                    where X is the input to the MHA layer, W^V is projection matrix for V(a.k.a valueWeights),
                    dL/dV is the gradient_value(calculated in step 4)
                 */
                Collective<t> gradient_value_weights = Numcy::matmul<t>(Numcy::transpose(this->X_ei_value), gradient_value);

                /*`                                                       
                    Learning Rate Scaling:
                    - During weight updates, we multiply the computed gradients by the learning rate.
                    - This controls the size of the update step taken towards minimizing the loss.
                    - A smaller learning rate means smaller updates (more stable but slower learning).
                    - A larger learning rate means bigger updates (faster but can cause instability if too large).
                    - Mathematically:  new_weight = old_weight - learning_rate * gradient
                 */
                gradient_query_weights = gradient_query_weights * learning_rate;
                this->queryWeights = this->queryWeights - gradient_query_weights;

                gradient_key_weights = gradient_key_weights * learning_rate;
                this->keyWeights = this->keyWeights - gradient_key_weights;

                gradient_value_weights = gradient_value_weights * learning_rate;
                this->valueWeights = this->valueWeights - gradient_value_weights;
                                
                /* 
                    Backpropogation: gradients flowing backwards from Q, K, V to their respectve Xs or inputs 
                    Chain Rule Logic in terms of Q and then same for K and V as well ...
                    - When Q = X.W^Q 
                      - W^Q is the projection matrix for Q, and it is constant  
                      - and X is the input to the MHA layer.                    
                    then dL/dX (changes in X affect the final loss) = dL/dQ * dQ/dX
                    there fore, dL/dX = dL/dQ * (W^Q)^T
                 */
                /*
                    Backpropagation: gradients flowing backward from Q, K, and V to their respective input X tensors.
                    Chain Rule Logic in terms of Q (similar for K and V as well):
                    
                    - When Q = X.W^Q:
                      - W^Q is the projection matrix for Q (learnable weights), considered constant during backpropagation at this point.
                      - X is the original input to the MHA layer.

                    Then, by chain rule:
                      dL/dX_from_Q (changes in X affect the final loss from Q side) = dL/dQ * (dQ/dX) 
                                                                                    = dL/dQ * (W^Q)^T

                    Similarly:
                      dL/dX_from_K (changes in X affect the final loss from K side) = dL/dK * (W^K)^T
                      dL/dX_from_V (changes in X affect the final loss from V side) = dL/dV * (W^V)^T

                    Since X was used three times to create Q, K, and V separately,
                    the total gradient with respect to X is the **sum** of these three contributions.
                    
                    That is:
                    
                    dL/dX = dL/dX (from Q path) + dL/dX (from K path) + dL/dX (from V path)

                    Finally, total dL/dX = dL/dX_from_Q + dL/dX_from_K + dL/dX_from_V
                    because the input X branches into three projections (Q, K, V) during the forward pass.
                 */
                /* Gradient of Loss w.r.t the input (X) that produced, Q(= X.W^Q) => dL/dX_query = dL/dQ * (W^Q)^T */
                Collective<t> input_gradient_from_query = Numcy::matmul(gradient_query, Numcy::transpose(this->queryWeights));
                /* Gradient of Loss w.r.t the input (X) that produced, K(= X.W^K) => dL/dX_key = dL/dK * (W^K)^T */
                Collective<t> input_gradient_from_key = Numcy::matmul(gradient_key, Numcy::transpose(this->keyWeights));
                /* Gradient of Loss w.r.t the input (X) that produced, V(= X.W^V) => dL/dX_value = dL/dV * (W^V)^T */
                Collective<t> input_gradient_from_value = Numcy::matmul(gradient_value, Numcy::transpose(this->valueWeights));
                
                // Summing all the gradients flowing into X
                /*incoming_gradient*/ input_gradient = input_gradient_from_query = input_gradient_from_query * learning_rate;
            } 
            catch (ala_exception& e) 
            {
                throw ala_exception(cc_tokenizer::String<char>("Attention::backward() -> ") + e.what());
            }
                        
            return /*incoming_gradient*/ input_gradient; // Placeholder return value
        }

```
